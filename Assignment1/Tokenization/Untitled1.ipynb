{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5373b13",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "\n",
    "NLP is a set of steps followed to build an end to end NLP software. \n",
    "\n",
    "NLP software consists of the following steps: \n",
    "\n",
    "- Data Acquistion : Data prepartion always required\n",
    "- Text Preparation \n",
    "  - Text Cleanup\n",
    "  - Basic Preprocessing\n",
    "  - Advance Preprocessing \n",
    "- Feature Engineering : Data in correct format for applying algorithms\n",
    "- Modelling \n",
    "  - Model Building\n",
    "  - Evaluation\n",
    "- Deployment\n",
    "  - Deployment\n",
    "  - Monitoring\n",
    "  - Model Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca685d62",
   "metadata": {},
   "source": [
    "### Points to Remember\n",
    "- It is not universal\n",
    "- Deep learning pipelines are slightly different\n",
    "- Pipeline is non-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb78ed6",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "There are three things possible for the data : \n",
    "1. Either it is available to you.\n",
    "2. It is available to others.\n",
    "3. It is first time thing u r making , nobody has the data\n",
    "\n",
    "\n",
    "When you have less data available --> you perform data augmentation.\n",
    "**Available to you**\n",
    "\n",
    "**Data Augmentation** : Data augmentation is a set of techniques to artificially increase the amount of data by generating new data points from existing data. This includes making small changes to data or using deep learning models to generate new data points.\n",
    "\n",
    "Data augmentation is not as popular in the NLP domain as in the computer vision domain. Augmenting text data is difficult, due to the complexity of a language. Common methods for data augmentation in NLP are:\n",
    "\n",
    "- Easy Data Augmentation (EDA) operations: synonym replacement, word insertion, word swap and word deletion\n",
    "- Back translation: re-translating text from the target language back to its original language\n",
    "- Contextualized word embeddings\n",
    "- Bigram flip: translate to some other language and then again translate to previous one.\n",
    "\n",
    "**When available to others**\n",
    "- Public dataset: kaggle etc\n",
    "- Web scraping: beautiful soup etc\n",
    "- API: rapidAPI website where you can find apis for data\n",
    "- PDF: use libraries etc\n",
    "- Image: text extract from image , OCR etc\n",
    "- Audio: speect to text library \n",
    "\n",
    "**Nobody**\n",
    "Company allows its users for review kind of thing etc. Heavy process when u don't have any data.\n",
    "\n",
    "## Text preparation\n",
    "\n",
    "- Cleaning\n",
    "  - HTML tag cleaning\n",
    "  - emoji: encode it in machine understandable language and furthur can be used using feature engineering\n",
    "  - spelling check: fast finger , fat finger , libraries present\n",
    "  \n",
    "- Basic Preprocessing\n",
    "  - Basic: Tokenization \n",
    "  - Optional : Stopwords removal, Stemming , Removing punctuation, digits, Lowercasing , Language detection \n",
    "  \n",
    "- Advance Preprocessing\n",
    "  - Parts of Speech of tagging\n",
    "  - Parsing \n",
    "  - Coreference resolution\n",
    "  \n",
    "## Feature Engineering\n",
    "- Preaparing data for applying models. Converting textual data to desired form like in numbers , vectors .\n",
    "- Many approaches are there like counting positive words, count\n",
    "\n",
    "## Modelling \n",
    "\n",
    "**Modelling** - Heuristic , ML algo , Deep Learning , Cloud API , Transfer Learning( already trained model somwhere can be used) . Can use one of them. Depends on \n",
    "  - Amount of data\n",
    "  - Nature of problem\n",
    "\n",
    "\n",
    "**Evaluation** - Intrinsic eval : technicality evaluation , checking precision etc\n",
    "               - Extrinsic eval : after deployment based on reviews etc\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55312430",
   "metadata": {},
   "source": [
    "## Text Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6c67a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HTML Tags remove\n",
    "import re \n",
    "import string,time\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "## remove URLs\n",
    "### www.google.com etc\n",
    "\n",
    "## ? means last letter is optional , S says any non white space character , | means or \n",
    "\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return patter.sub(r'', text)\n",
    "\n",
    "## remove punctuations\n",
    "\n",
    "# print(string.punctuation)!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}\n",
    "# 1, \n",
    "def remove_punctuations(text):\n",
    "    for char in string.punctuation:\n",
    "        text = text.replace(char,'')\n",
    "    return text\n",
    "# 2,faster\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "\n",
    "## chat word treatment\n",
    "## asap , lmao, gn, gm, imho etc\n",
    "## required when chat type data is taken\n",
    "\n",
    "def chat_conversion(text):\n",
    "    new_txt = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_txt.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_txt.append(w)\n",
    "    return new_txt\n",
    "\n",
    "\n",
    "## spelling correction\n",
    "\n",
    "from textblob import TextBlob\n",
    "textBlb = TextBlob(incorrect_text)\n",
    "textBlb.correct().string\n",
    "\n",
    "# remove stopwords\n",
    "# not required when sentiment analysis or documebt classification\n",
    "# required in case of POS tagging\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def rem_stowords(text):\n",
    "    new_txt = []\n",
    "    for w in text.split():\n",
    "        if w in stopwords.words('english'):\n",
    "            new_txt.append('')\n",
    "        else: \n",
    "            new_txt.append(w)\n",
    "    x = new_txt[:]\n",
    "    new_txt.clear()\n",
    "    return \" \".join(x)\n",
    "        \n",
    "\n",
    "## handling emojis\n",
    "## we can remove with checking textual form , or replace with a word like happy , sad\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'',text)\n",
    "\n",
    "# replacing with word\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c324106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyton is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Pyton is ðŸ”¥'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf23408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation\n",
    "\n",
    "# breaking text documents into smaller parts\n",
    "# word tokenization \n",
    "# sentence tokenisation ,\n",
    "# usually we do word tokenization , in word tokenization there are anomalies which can happen\n",
    "# prefix : $(\" \n",
    "# suffix: km ) , . !\n",
    "# infix - -- / ...\n",
    "# special case rule to split a string into several tokens or prevent a token from being split when puncutation rules are apllies\n",
    "# Ph.D U.S. \n",
    "\n",
    "\n",
    "# Libraries nltk , spacy\n",
    "\n",
    "\n",
    "# Stemming \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

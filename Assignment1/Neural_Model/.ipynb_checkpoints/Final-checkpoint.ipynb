{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from Token import Clean\n",
    "from Token import Tokenise,paddingString\n",
    "from nltk.util import ngrams\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "\n",
    "filePath = '../DATA/Pride and Prejudice - Jane Austen.txt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_data, batch_size, min_freq=5):\n",
    "        self.data = train_data\n",
    "        self.max_len = 30\n",
    "        self.min_freq = min_freq\n",
    "        self.vocab = []\n",
    "        self.batch_size = batch_size\n",
    "        self.ngramList = []\n",
    "        sents = self.loadingWords()\n",
    "        self.wordToIndex = {w: i for i, w in enumerate(self.vocab)}\n",
    "        self.indexToWord = {i: w for i, w in enumerate(self.vocab)}\n",
    "        self.padIndex = self.wordToIndex['<PAD>']\n",
    "        self.unKnownIndex = self.wordToIndex['<UNK>']\n",
    "        self.startIndex = self.wordToIndex['<START>']\n",
    "        self.endIndex = self.wordToIndex['<END>']\n",
    "        for sent in sents:\n",
    "            tokens = sent\n",
    "            prefix_seqs = []\n",
    "            try:\n",
    "                pfx = [tokens[0]]\n",
    "                for token in tokens[1:]:\n",
    "                    pfx.append(token)\n",
    "                    prefix_seqs.append(pfx.copy())\n",
    "            \n",
    "                for i in range(len(prefix_seqs)):\n",
    "                    currSeq = [self.wordToIndex.get(w,self.unKnownIndex) for w in prefix_seqs[i]]\n",
    "                    pref_sq = [self.startIndex] + [self.padIndex]*(self.max_len-len(currSeq)) + [w for w in currSeq]\n",
    "                    self.ngramList.append(list(pref_sq))\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "    def loadingWords(self):\n",
    "        text = [line for line in self.data if line.strip()]\n",
    "        sentences = []\n",
    "        wordFreq = {}\n",
    "        mx = 0\n",
    "        for line in text:\n",
    "            pd = paddingString(line)\n",
    "            tokens = Tokenise(pd)\n",
    "            sentences.append(tokens)\n",
    "            self.vocab += tokens\n",
    "            mx = max(mx, len(tokens))\n",
    "            for token in tokens:\n",
    "                if token in wordFreq:\n",
    "                    wordFreq[token] += 1\n",
    "                else:\n",
    "                    wordFreq[token] = 1\n",
    "\n",
    "        # wordCount = Counter(wordFreq)\n",
    "        wordCount = {}\n",
    "        self.vocab = list(filter(lambda w: wordFreq[w] >= self.min_freq, self.vocab))\n",
    "        print(wordFreq['kind'])\n",
    "        self.vocab = ['<PAD>', '<UNK>', '<START>', '<END>'] + self.vocab\n",
    "        self.vocab = set(self.vocab)\n",
    "        self.vocabSize = len(self.vocab)\n",
    "        print(self.vocabSize)\n",
    "        self.max_len = max(mx,self.max_len)\n",
    "        # print(sentences)\n",
    "        return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "1700\n",
      "15\n",
      "518\n",
      "9\n",
      "525\n"
     ]
    }
   ],
   "source": [
    "def splitData(corpus,train_ratio,valid_ratio,test_ratio):\n",
    "    with open(corpus, 'r') as f:\n",
    "        text = f.readlines()\n",
    "    text = [line.strip() for line in text if line.strip()]\n",
    "    train_size = int(len(text) * train_ratio)\n",
    "    valid_size = int(len(text) * valid_ratio)\n",
    "    test_size = int(len(text) * test_ratio)\n",
    "    train_data = text[:train_size]\n",
    "    valid_data = text[train_size:train_size + valid_size]\n",
    "    test_data = text[train_size + valid_size:]\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "train_data, valid_data, test_data = splitData(filePath,0.7,0.15,0.15)\n",
    "TRAINSET = Dataset(train_data, 256)\n",
    "VLAIDSET = Dataset(valid_data, 256)\n",
    "TESTSET = Dataset(test_data, 256)\n",
    "#TRAINSET.wordToIndex['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBatch(dataset):\n",
    "    input_ngram,trg = [],[]\n",
    "    for tg in dataset:\n",
    "        input_ngram.append(tg[:-1])\n",
    "        trg.append(tg[-1])\n",
    "    return torch.tensor(input_ngram,dtype=torch.long),torch.tensor(trg,dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TRAINSET.ngramList, batch_size=256, shuffle=True, collate_fn=generateBatch)\n",
    "valid_loader = DataLoader(VLAIDSET.ngramList, batch_size=256, shuffle=True, collate_fn=generateBatch)\n",
    "test_loader = DataLoader(TESTSET.ngramList, batch_size=256, shuffle=True, collate_fn=generateBatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "class LSTMmodel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers, vocabSize,dropout):\n",
    "        super(LSTMmodel, self).__init__()\n",
    "        self.vocab_size = vocabSize\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size, self.embedding_size, device=device)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_size,\n",
    "                            hidden_size=self.hidden_size, batch_first=True, device=device)\n",
    "        self.dropLayer = nn.Dropout(p=self.dropout)\n",
    "        self.output = nn.Linear(\n",
    "            self.hidden_size, self.vocab_size, bias=False, device=device)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, xContext):\n",
    "        xembed = self.dropLayer(self.embedding(xContext))\n",
    "        out, hidden = self.lstm(xembed)\n",
    "        out = self.log_softmax(self.output(out[:,-1]))\n",
    "        #out = self.output(out[:,-1])\n",
    "        return out, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "# from dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from Model import LSTMmodel\n",
    "import sys\n",
    "# from Token import Clean\n",
    "# from Token import Tokenise\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, model:nn.Module,epochs,datasetTrain:torch.utils.data.DataLoader,datasetValid:torch.utils.data.DataLoader,datasetTest:torch.utils.data.DataLoader):\n",
    "        self.model = model\n",
    "        self.datasetTrain = datasetTrain\n",
    "        self.datasetValid = datasetValid\n",
    "        self.datasetTest = datasetTest\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.epochs = epochs\n",
    "        self.clip = 1\n",
    "        self.patience = 10\n",
    "        self.learning_rate = 1e-3\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.learning_rate,amsgrad = True)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=6, gamma=0.1,last_epoch=-1,verbose=False)\n",
    "\n",
    "    def train(self):\n",
    "        maxValidLoss = math.inf\n",
    "        ctr = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epochAcc = 0\n",
    "            epochLoss = 0\n",
    "            self.model.train()\n",
    "            # hidden = self.model.init_hidden(24)\n",
    "            for i, (x, y) in enumerate(tqdm(self.datasetTrain)):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs, hidden = self.model(x)\n",
    "                y = y.view(-1)\n",
    "                loss = self.criterion(outputs, y)\n",
    "                loss.backward()\n",
    "\n",
    "                epochAcc += 100*(outputs.argmax(dim=1)==y).sum().item()/y.shape[0]\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "                epochLoss += loss.item()\n",
    "                self.optimizer.step()\n",
    "                if i % 100 == 0:\n",
    "                    print(\n",
    "                        f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\")\n",
    "                    \n",
    "            print(f\"Epoch: {epoch}, Loss: {epochLoss/len(self.datasetTrain)}, Accuracy: {epochAcc/len(self.datasetTrain)}\")\n",
    "            # validate\n",
    "            valid_loss = self.validate()\n",
    "            self.scheduler.step()\n",
    "            if valid_loss < maxValidLoss:\n",
    "                maxValidLoss = valid_loss\n",
    "                torch.save(self.model.state_dict(), 'model2.pt')\n",
    "                print(\"Model saved\")\n",
    "                ctr = 0\n",
    "            else:\n",
    "                ctr += 1\n",
    "                print(f\"Validation loss not improved for {ctr} epochs\")\n",
    "            if ctr > self.patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        epochAcc = 0\n",
    "        epochLoss = 0\n",
    "        for i, (x, y) in enumerate(tqdm(self.datasetValid)):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs, hidden = self.model(x)\n",
    "            y = y.view(-1)\n",
    "            loss = self.criterion(outputs, y)\n",
    "            epochAcc += 100*(outputs.argmax(dim=1)==y).sum().item()/y.shape[0]\n",
    "            epochLoss += loss.item()\n",
    "\n",
    "        print(f\"Validation Loss: {epochLoss/len(self.datasetValid)}, Validation Accuracy: {epochAcc/len(self.datasetValid)}\")\n",
    "        return epochLoss/len(self.datasetValid)\n",
    "    \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        epochAcc = 0\n",
    "        epochLoss = 0\n",
    "        for i, (x, y) in enumerate(tqdm(self.datasetTest)):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs, hidden = self.model(x)\n",
    "            y = y.view(-1)\n",
    "            loss = self.criterion(outputs, y)\n",
    "            epochAcc += 100*(outputs.argmax(dim=1)==y).sum().item()/y.shape[0]\n",
    "            epochLoss += loss.item()\n",
    "\n",
    "        print(f\"Test Loss: {epochLoss/len(self.datasetTest)}, Test Accuracy: {epochAcc/len(self.datasetTest)}\")\n",
    "        return epochLoss/len(self.datasetTest)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = TRAINSET.vocabSize\n",
    "EMBEDDING_DIM = 512\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROP_OUT = 0.5\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "lngMOD = LSTMmodel(EMBEDDING_DIM,HIDDEN_DIM,NUM_LAYERS, VOCAB_SIZE, DROP_OUT)\n",
    "eval = Evaluation(lngMOD,20,train_loader,valid_loader,test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/373 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [335]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [333]\u001b[0m, in \u001b[0;36mEvaluation.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# hidden = self.model.init_hidden(24)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasetTrain)):\n\u001b[0;32m---> 43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "eval.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 157/157 [00:02<00:00, 53.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 7.395227067789454, Test Accuracy: 14.464143539581437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.395227067789454"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalVocab = TRAINSET.vocab\n",
    "globalWordToIndex = TRAINSET.wordToIndex\n",
    "PAD_INDEX = TRAINSET.padIndex\n",
    "START_INDEX = TRAINSET.startIndex\n",
    "UNK_INDEX = TRAINSET.unKnownIndex\n",
    "MAX_LEN = TRAINSET.max_len\n",
    "\n",
    "def getProbPerplexity(model,dataset):\n",
    "    model.eval()\n",
    "    perplexity_list = []\n",
    "    with torch.no_grad():\n",
    "        for line in dataset.data:\n",
    "            perplexity = perpForSentence(model,line)\n",
    "            if perplexity != -1:\n",
    "            \tperplexity_list.append({'line':line,'perplexity':perplexity})\n",
    "            \n",
    "    # averagePerplexty\n",
    "    avgPerplexity = sum([line['perplexity'] for line in perplexity_list])/len(perplexity_list)\n",
    "    return perplexity_list,avgPerplexity\n",
    "\n",
    "def writeToFile(filePath,perplexity_list,avg):\n",
    "    with open(filePath,'w') as f:\n",
    "        f.write(f\"Average Perplexity: {avg}\\n\")\n",
    "        for line in perplexity_list:\n",
    "            f.write(f\"{line['line']}\\t {line['perplexity']}\\n\")\n",
    "            \n",
    "            \n",
    "def perpForSentence(model,sentence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prob_gram = 1\n",
    "        tokens = Tokenise(sentence)\n",
    "        print(tokens)\n",
    "        if len(tokens)==0:\n",
    "            return -1\n",
    "        elif len(tokens) == 1:  # handle unigram case\n",
    "            input_gram = torch.tensor([globalWordToIndex.get(tokens[0],UNK_INDEX)],dtype=torch.long).to(device)\n",
    "            output,hidden = model(input_gram.unsqueeze(dim=0))\n",
    "            output = torch.exp(output.view(-1))\n",
    "            prob_gram = prob_gram*output[input_gram.item()].cpu().numpy()\n",
    "            perplexity = (1/prob_gram)**(1/len(tokens))\n",
    "            return perplexity\n",
    "        elif len(tokens)>1:\n",
    "            prefix_seqs = []\n",
    "            gramList = []\n",
    "            try:\n",
    "                pfx = [tokens[0]]\n",
    "                #prefix_seqs.append(pfx.copy())\n",
    "                for token in tokens[1:]:\n",
    "                    pfx.append(token)\n",
    "                    prefix_seqs.append(pfx.copy())\n",
    "                for i in range(len(prefix_seqs)):\n",
    "                    currSeq = [globalWordToIndex.get(w,UNK_INDEX) for w in prefix_seqs[i]]\n",
    "                    pref_sq = [START_INDEX] + [PAD_INDEX]*(MAX_LEN-len(currSeq)) + [w for w in currSeq]\n",
    "                    gramList.append(list(pref_sq))\n",
    "            except IndexError:\n",
    "                print(tokens)\n",
    "                print(\"idhr 2\")\n",
    "                return float(\"NaN\")\n",
    "\n",
    "            if len(gramList)>0:\n",
    "                for gram in gramList:\n",
    "                    input_gram = torch.tensor(gram[:-1],dtype=torch.long).to(device)\n",
    "                    output_gram = gram[-1]\n",
    "                    output,hidden = model(input_gram.unsqueeze(dim=0))\n",
    "                    output = torch.exp(output.view(-1))\n",
    "                    prob_gram = prob_gram * output[output_gram].cpu().numpy()\n",
    "\n",
    "                    perplexity = (1/prob_gram)**(1/len(gramList))\n",
    "                return perplexity\n",
    "            else:\n",
    "                print(\"idhr\")\n",
    "                return float(\"NaN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "perplexity_list, avgPerplexity = getProbPerplexity(lngMOD,TESTSET)\n",
    "path = 'test2_perplexity.txt'\n",
    "writeToFile(path,perplexity_list,avgPerplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'long', 'is', 'haines', 'going', 'to', 'stay', 'in', 'this', 'tower']\n",
      "90.7964180689187\n",
      "['warrior']\n",
      "56.674560294432325\n"
     ]
    }
   ],
   "source": [
    "str1 = \"—How long is Haines going to stay in this tower?\"\n",
    "str2 = \"warrior\"\n",
    "\n",
    "print(perpForSentence(lngMOD,str1))\n",
    "print(perpForSentence(lngMOD,str2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = TRAINSET.max_len\n",
    "MAX_LEN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7781fbed3122e90f9621d444eb681da4e516177f623617ad7dd98727908d5192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
